<h1>A brief introduction to Neural Networks</h1>
<p>P.S. I will regularly update this repo, but now will start will the differences between the activation functoins</p>

<h3>Relu Vs Sigmoid</h3>
The shape of sigmoid activation function follows a positive logrithimic function above 0 and negative log below 0 compared to relu that follows a straight line positive slope after a certain point and is zero before that. 

<b>However,Towards the edges of the sigmoid function the slope falls down to zero that drastically reduces gradient to nearly 0 and in turn reducing learning times when compared to the relu function. </b> 

![Relu](https://miro.medium.com/max/1400/1*XxxiA0jJvPrHEJHD4z893g.png)